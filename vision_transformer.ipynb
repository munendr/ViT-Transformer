{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "with open(\"helper_functions.py\", \"wb\") as f:\n",
    "    request =  requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/refs/heads/main/helper_functions.py\")\n",
    "    f.write(request.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_functions import download_data\n",
    "image_path = download_data(source = \"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\",\n",
    "                           destination = \"pizza_steak_sushi\")\n",
    "image_path "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = image_path / \"train\"\n",
    "test_dir = image_path / \"test\"\n",
    "train_dir, test_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from going_modular import data_setup\n",
    "\n",
    "IMAGE_SIZE = 224\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "#creatig transform pipeline\n",
    "manual_transforms = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    torchvision.transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(train_dir=train_dir,\n",
    "                                                                                test_dir = test_dir,\n",
    "                                                                                batch_size = BATCH_SIZE,\n",
    "                                                                                transform= manual_transforms)\n",
    "len(train_dataloader), len(test_dataloader), class_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a batch of image\n",
    "image_batch, label_batch = next(iter(train_dataloader))\n",
    "image, label = image_batch[0], label_batch[0]\n",
    "image.shape, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_batch.shape, label_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(image.permute(1, 2, 0))\n",
    "plt.title(class_names[label])\n",
    "plt.axis(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create example values\n",
    "height = 224\n",
    "width = 224\n",
    "color_channels = 3\n",
    "patch_size = 16\n",
    "\n",
    "# calculate the number of patches\n",
    "number_of_patches = int((height * width)/ patch_size ** 2)\n",
    "number_of_patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input shape\n",
    "embedding_layer_input_shape = (height, width, color_channels)\n",
    "embedding_layer_output_shape = (number_of_patches, patch_size ** 2 * color_channels) #output shpae = (n , P^2*c)\n",
    "print(f\"Input shape (single 2D image): {embedding_layer_input_shape}\")\n",
    "print(f\"Output shape (single 1D sequence of patches): {embedding_layer_output_shape} -> (number_of_patches, embedding_dimension)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#equation : 1 - split data into patches and creating the class, position and patch embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(image.permute(1, 2, 0))\n",
    "plt.title(class_names[label])\n",
    "plt.axis(False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_permuted = image.permute(1, 2, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting whole image as patches\n",
    "img_size = 224\n",
    "patch_size = 16\n",
    "num_patches = img_size // patch_size\n",
    "assert img_size % patch_size == 0, \"Image size must be divisible by patch_size\"\n",
    "print(f\"Number of patches per row: {num_patches} \\\n",
    "      \\nNumber of patches per column : {num_patches} \\\n",
    "      \\nTotal patches : {num_patches * num_patches} \\\n",
    "      \\nPatch size : {patch_size} pixels x {patch_size} pixels\")\n",
    "\n",
    "# create a series of subplots\n",
    "fig, axs = plt.subplots(nrows = img_size // patch_size,\n",
    "                       ncols = img_size // patch_size,\n",
    "                       figsize = (num_patches, num_patches),\n",
    "                       sharex = True,\n",
    "                       sharey = True)\n",
    "# loop through height and width of image\n",
    "for i, patch_height in enumerate(range(0, img_size, patch_size)):\n",
    "    for j, patch_width in enumerate(range(0, img_size, patch_size)):\n",
    "      axs[i, j].imshow(image_permuted[patch_height:patch_height + patch_size, # iterate through height\n",
    "                                      patch_width:patch_width + patch_size, # iterate through width\n",
    "                                      :]) # get all color channels\n",
    "      axs[i, j].set_ylabel(i+1,\n",
    "                           rotation = 'horizontal',\n",
    "                           horizontalalignment = 'right',\n",
    "                           verticalalignment = 'center')\n",
    "      axs[i, j].set_xlabel(j + 1)\n",
    "      axs[i, j].set_xticks([])\n",
    "      axs[i, j].set_yticks([])\n",
    "      axs[i, j].label_outer()\n",
    "\n",
    "#set the title of the plot\n",
    "fig.suptitle(f\"{class_names[label]} -> Patchified\", fontsize = 14)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "patch_size = 16\n",
    "\n",
    "conv2d = nn.Conv2d(in_channels = 3, # for color images\n",
    "                   out_channels = 768, #D size from table of paper or (p^2 * c ) \n",
    "                   kernel_size= patch_size,\n",
    "                   stride = patch_size,\n",
    "                   padding = 0)\n",
    "conv2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_out_of_conv = conv2d(image.unsqueeze(dim = 0))\n",
    "img_out_of_conv.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot random convolutional feature maps(embeddings)\n",
    "import random\n",
    "random_indexes = random.sample(range(0, 768), k = 5)\n",
    "print(f\"Showing random convolutional feature maps from indexes : {random_indexes}\")\n",
    "# create plot\n",
    "fig, axs = plt.subplots(nrows = 1, ncols = 5, figsize = (12 ,12))\n",
    "\n",
    "# plot random feature maps\n",
    "for i, idx in enumerate(random_indexes):\n",
    "    image_conv_feature_map = img_out_of_conv[:, idx, :, :] # index on the output tensor of the conv2d layer\n",
    "    axs[i].imshow(image_conv_feature_map.squeeze().detach().numpy()) # removes batch dimension(squeeze()), remove from gradient tracking (detach()) and switch to numpy (numpy)\n",
    "    axs[i].set(xticklabels = [], \n",
    "               yticklabels = [],\n",
    "               xticks = [],\n",
    "               yticks = [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_feature_map = img_out_of_conv[:, 0, :, :]\n",
    "single_feature_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "flatten_layer = nn.Flatten(start_dim = 2,\n",
    "                           end_dim = 3)\n",
    "print(f\"{flatten_layer(img_out_of_conv).shape} -> (batch_size, embedding_dimension, no. of patches)\")\n",
    "print(f\"Wanted order (batch_size, no. of patches, embeddin_dimension)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(image.permute(1, 2, 0))\n",
    "plt.title(class_names[label])\n",
    "plt.axis(False)\n",
    "print(f\"Original shape : {image.shape}\")\n",
    "\n",
    "# Turn image into feature maps\n",
    "image_out_of_conv = conv2d(image.unsqueeze(dim = 0)) # add batch dimension\n",
    "print(f\"Image featue map (patches) shape : {img_out_of_conv.shape}\")\n",
    "\n",
    "# Flatten the feature map\n",
    "image_out_of_flattened = flatten_layer(img_out_of_conv)\n",
    "print(f\"Flattened image feature map shape: {image_out_of_flattened.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rearrage output of flattened layer\n",
    "print(f\"{image_out_of_flattened.permute(0, 2, 1).shape} -> (batch_size, number_of_patches, embedding dimension)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_out_of_flattened_pemuted = image_out_of_flattened.permute(0, 2, 1)\n",
    "image_out_of_flattened_pemuted.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a single flattened feature map\n",
    "single_flattened_feature_map = image_out_of_flattened_pemuted[:, :, 0]\n",
    "\n",
    "# plot the flattened map visually \n",
    "plt.figure(figsize= (22, 22))\n",
    "plt.imshow(single_flattened_feature_map.detach().numpy())\n",
    "plt.title(f\"Flattened feature map shape: {single_flattened_feature_map.shape}\")\n",
    "plt.axis(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_image(image, patch_size = None):\n",
    "    print(f\"Input shape of given image {image.shape}\")\n",
    "    if patch_size:\n",
    "        OUT_CHANNELS = patch_size * patch_size * 3\n",
    "    else:\n",
    "        OUT_CHANNELS = 768\n",
    "    conv2d = torch.nn.Conv2d(in_channels = 3,\n",
    "                             out_channels = OUT_CHANNELS,\n",
    "                             kernel_size = patch_size,\n",
    "                             stride = patch_size,\n",
    "                             padding = 0)\n",
    "    a = conv2d(image.unsqueeze(dim = 0))\n",
    "    print(f\"shape a : {a.shape}\")\n",
    "    flatten_layer = torch.nn.Flatten(start_dim = 2,\n",
    "                                     end_dim = -1)\n",
    "    b = flatten_layer(a)\n",
    "    print(b.shape)\n",
    "    c = b.permute(0, 2, 1)\n",
    "    print(c.shape)\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = final_image(image, patch_size=16)\n",
    "c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_channels:int  = 3,\n",
    "                 patch_size: int = 16,\n",
    "                 embedding_dim : int = 768):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.patcher = nn.Conv2d(in_channels = in_channels,\n",
    "                                 out_channels = embedding_dim,\n",
    "                                 kernel_size = patch_size,\n",
    "                                 stride = patch_size,\n",
    "                                 padding = 0)\n",
    "        self.flattened = nn.Flatten(start_dim = 2,\n",
    "                                  end_dim = 3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #check input are correct shape\n",
    "        image_resolution = x.shape[-1]\n",
    "        assert image_resolution % patch_size == 0, f\"Input image size must be divisible by patch size , image shape: {image_resolution}, patch_size: {self.patch_size}\"\n",
    "        x_patched = self.patcher(x)\n",
    "        x_flattened = self.flattened(x_patched)\n",
    "        \n",
    "        # maker sure the returned sequence embedding dimensions are in the right order (batch_size, number of patches, embedding_dimension)\n",
    "        return x_flattened.permute(0, 2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.manual_seed(42)\n",
    "torch.manual_seed(42)\n",
    "patchify = PatchEmbedding(in_channels = 3, \n",
    "                          patch_size = 16,\n",
    "                          embedding_dim = 768)\n",
    "print(f\"Input image size: {image.unsqueeze(dim = 0).shape}\")\n",
    "patch_embedding_image = patchify(image.unsqueeze(0))\n",
    "print(f\"Output patch embedding sequence shape: {patch_embedding_image.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = patch_embedding_image.shape[0]\n",
    "embedding_dimension = patch_embedding_image.shape[2]\n",
    "batch_size, embedding_dimension\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create class token embedding as a learnable parameter that shares the same size as the embedding dimension(D)\n",
    "class_token = nn.Parameter(torch.ones(batch_size, 1, embedding_dimension), # torch.randn() is used, torch.ones() to understand only\n",
    "                           requires_grad = True)\n",
    "class_token.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_embedded_image_with_class_embedding = torch.cat((class_token, patch_embedding_image),\n",
    "                                                      dim = 1) # no. of patches dimension\n",
    "print(patch_embedded_image_with_class_embedding)\n",
    "print(f\"Sequence of patch embeddings with class token prepended {patch_embedded_image_with_class_embedding.shape} -> (batch_size, class_token + no_of_patches, embedding_dimension)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the position embedding\n",
    "# create a series of 1D learnable position embeddings and to add them to the sequence of patch embedding\n",
    "# NOTE: after adding the position embedding shape will remain the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_embedded_image_with_class_embedding, patch_embedded_image_with_class_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_patches = int((height * width) / patch_size **2)\n",
    "number_of_patches\n",
    "embedding_dimension = patch_embedded_image_with_class_embedding.shape[-1]\n",
    "embedding_dimension\n",
    "position_embedding = nn.Parameter(torch.ones(1,\n",
    "                                             number_of_patches + 1,\n",
    "                                             embedding_dimension),\n",
    "                                    requires_grad = True)\n",
    "position_embedding, position_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_and_position_embedding = patch_embedded_image_with_class_embedding + position_embedding\n",
    "patch_and_position_embedding, patch_and_position_embedding.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myfunction_put_it_all_together_eqn1(image, patch_size):\n",
    "    print(f\"Shape of input image: {image.shape}\")\n",
    "    assert (image.shape[1] * image.shape[2]) % patch_size ** 2 == 0 ,f\"Input image_size must be divisible by patch_size\"\n",
    "    no_of_patches = int(image.shape[1] * image.shape[2] / patch_size ** 2)\n",
    "    print(f\"No. of patches = {no_of_patches}\")\n",
    "    conv = torch.nn.Conv2d(in_channels=image.shape[0],\n",
    "                           out_channels= (patch_size ** 2 ) * image.shape[0],\n",
    "                           kernel_size= patch_size,\n",
    "                           stride = patch_size,\n",
    "                           padding = 0)(image)\n",
    "    #print(conv.shape)\n",
    "    flat = torch.nn.Flatten(start_dim = 1, end_dim = -1)(conv)\n",
    "    #print(flat.shape)\n",
    "    flat = flat.permute(1, 0).unsqueeze(dim = 0)\n",
    "    #print(flat.shape)\n",
    "    class_emb = torch.nn.Parameter(torch.ones(1, batch_size, (patch_size ** 2) * image.shape[0]),\n",
    "                                   requires_grad = True)\n",
    "    #print(class_emb.shape)\n",
    "    img_and_class_emb = torch.cat((class_emb, flat), dim = 1)\n",
    "    #print(img_and_class_emb.shape)\n",
    "    pos_emb = torch.nn.Parameter(torch.ones(1, img_and_class_emb.shape[1], (patch_size ** 2) * image.shape[0]))\n",
    "    print(pos_emb.shape)\n",
    "    final_embedding = img_and_class_emb + pos_emb\n",
    "    print(final_embedding)\n",
    "    print(f\"Final shape:{final_embedding.shape} -> (batch_size, class_name_emb + no. of batches, embedding_dimension)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myfunction_put_it_all_together_eqn1(image=image,\n",
    "                                    patch_size = 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Putting it all together\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "\n",
    "patch_size= 16\n",
    "\n",
    "# print shapes of the original image tensor and get the image dimensions\n",
    "print(f\"Image tensor shape: {image.shape}\")\n",
    "height, width = image.shape[1], image.shape[2]\n",
    "\n",
    "# get image tensor and add a batch dimension\n",
    "x = image.unsqueeze(0)\n",
    "print(f\"Input image shape: {x.shape}\")\n",
    "patch_embedding_layer = PatchEmbedding(in_channels = 3,\n",
    "                                       patch_size = patch_size,\n",
    "                                       embedding_dim = 768)\n",
    "# pass input throught patch embedding\n",
    "patch_embedding = patch_embedding_layer(x)\n",
    "print(f\"Patch embedding shape: {patch_embedding.shape}\")\n",
    "\n",
    "# create class token embedding\n",
    "batch_size = patch_embedding.shape[0]\n",
    "embedding_dimension = patch_embedding.shape[-1]\n",
    "class_token = nn.Parameter(torch.ones(batch_size, 1, embedding_dimension),\n",
    "                           requires_grad = True)\n",
    "print(f\"Class_token embedding shape: {class_token.shape}\")\n",
    "# prepend the class token embedding to patch embedding\n",
    "patch_embedding_class_token = torch.cat((class_token, patch_embedding), dim = 1)\n",
    "print(f\"Patch embedding with class token shape: {patch_embedding_class_token.shape}\")\n",
    "\n",
    "#create 1D learnable parameters(position embedding)\n",
    "number_of_patches = int((height * width) / patch_size ** 2)\n",
    "print(f\"No of patchees {number_of_patches}\")\n",
    "position_embedding = nn.Parameter(torch.ones(1, number_of_patches + 1, embedding_dimension),\n",
    "                                  requires_grad = True)\n",
    "print(f\"position_embedding shape : {position_embedding.shape}\")\n",
    "# add the position embedding to patch embedding with class token\n",
    "patch_and_position_embedding = patch_embedding_class_token + position_embedding\n",
    "print(f\"Patch adn position embedding shape: {patch_and_position_embedding.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turning equation 2 to code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MUltiHeadAtttentionBlock(nn.Module):\n",
    "    ''' Creates a multi-head self-attention block (MSA block for short)'''\n",
    "    def __init__(self,\n",
    "                 embedding_dim: int = 768, # hidden size D\n",
    "                 num_heads : int = 12, # heads from table 1\n",
    "                 attn_dropout : float = 0):\n",
    "        super().__init__()\n",
    "        \n",
    "        #create the norm layer(LN)\n",
    "        self.layer_norm = nn.LayerNorm(normalized_shape = embedding_dim)\n",
    "\n",
    "        # create multihead attention (MSA) layer\n",
    "\n",
    "        self.multihead_attn = nn.MultiheadAttention(embed_dim = embedding_dim,\n",
    "                                                    num_heads= num_heads,\n",
    "                                                    dropout = attn_dropout,\n",
    "                                                    batch_first = True # is the batch first? (batch, seq, features) -> (batch, number_of_patches, embedding_dimension)\n",
    "                                                    )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.layer_norm(x)\n",
    "        attn_output, _ = self.multihead_attn(query = x,\n",
    "                                             key = x,\n",
    "                                             value = x,\n",
    "                                             need_weights = False, # is specified returns attn output weights in addition to attn outputs\n",
    "                                             )\n",
    "        return attn_output    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance MSA block\n",
    "multihead_self_attention_block = MUltiHeadAtttentionBlock(embedding_dim= 768,\n",
    "                                                          num_heads= 12,\n",
    "                                                          attn_dropout= 0)\n",
    "\n",
    "# pass the patch and position image embedding sequence through MSA block\n",
    "patched_image_through_msa_block = multihead_self_attention_block(patch_and_position_embedding)\n",
    "print(f\"Input shape of MSA block: {patch_and_position_embedding.shape}\")\n",
    "print(f\"Ouput shape of MSA block : {patched_image_through_msa_block.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_and_position_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patched_image_through_msa_block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replicating equation 3: multilayer perceptron block(MLP Block)\n",
    "# the mlp contains two layers with a GELU or non-linearity.\n",
    "# GELU - (Gaussian Linear unit or GELU) is an activation function.\n",
    "# GELU can be thought as smoother relu\n",
    "# NOTE: layers can mean: fully-connected , dense, linear, feed-forward, all are often similar names for the same thing. \n",
    "# In PyTorch, they're often called `torch.nn.linear()` and in tensorflow they might be called `tf.keras.layers.Dense()`\n",
    "\n",
    "# MLP no of hidden units = MLP size in table 1\n",
    "# MLP \n",
    "# x = linear -> non-linear -> dropout -> linear -> dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPBlock(nn.Module):\n",
    "    def __init__(self,\n",
    "                 embedding_dim : int = 768,\n",
    "                 mlp_size : int = 3072,\n",
    "                 dropout : float = 0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        # create the norm layer(LN)\n",
    "        self.layer_norm = torch.nn.LayerNorm(normalized_shape= embedding_dim)\n",
    "\n",
    "        # create the mlp\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(in_features = embedding_dim,\n",
    "                      out_features = mlp_size),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(p = dropout),\n",
    "            nn.Linear(in_features= mlp_size,\n",
    "                      out_features = embedding_dim),\n",
    "            nn.Dropout(p = dropout),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.layer_norm(x)\n",
    "        x = self.mlp(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_block = MLPBlock(embedding_dim = 768,\n",
    "                     mlp_size = 3072,\n",
    "                     dropout = 0.1)\n",
    "patched_image_through_mlp_block = mlp_block(patched_image_through_msa_block)\n",
    "print(f\"INput shape of MLP block: {patched_image_through_msa_block.shape}\")\n",
    "print(f\"Output shape of MSA block: {patched_image_through_mlp_block.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformer encoder is the combination of alternating layers of equation 2 and equation 3\n",
    "# residual connections = add a layer(s) input to its subsequent output, this enables the creation of deeper networkrs(prevent weights from getting too small (gradient vanishing))\n",
    "\n",
    "# transformer encoder\n",
    "# x_input -> MSA_block -> [MSA_block_output + x_input] -> MLP_block -> [MLP_block_output + MSA_block_output + x_input] -> ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderBlock(nn.Module):\n",
    "    def __init__(self,\n",
    "                 embedding_dim : int = 768,\n",
    "                 num_heads : int = 12,\n",
    "                 mlp_size : int  = 3072,\n",
    "                 mlp_dropout: int = 0.1,\n",
    "                 attn_dropout: int = 0):\n",
    "        super().__init__()\n",
    "\n",
    "        # create MSA block (equation 2)\n",
    "        self.msa_block = MUltiHeadAtttentionBlock(embedding_dim=embedding_dim,\n",
    "                                                  num_heads=num_heads,\n",
    "                                                  attn_dropout = attn_dropout)\n",
    "        \n",
    "        # create MLP block (equation 3)\n",
    "        self.mlp_block = MLPBlock(embedding_dim = embedding_dim,\n",
    "                                  mlp_size = mlp_size,\n",
    "                                  dropout = mlp_dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.msa_block(x) + x # residual/skip connection for equation 2\n",
    "        x = self.mlp_block(x) + x # residual/skip connection for equation 3\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_encoder_block = TransformerEncoderBlock()\n",
    "import torchinfo\n",
    "torchinfo.summary(model= transformer_encoder_block,\n",
    "                  input_size = (1, 197, 768), #(batch_size, number_of_patches, embedding_dimension)\n",
    "                  col_names =['input_size', 'output_size', 'num_params', 'trainable'],\n",
    "                  col_width = 20,\n",
    "                  row_settings = ['var_names'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_transformer_encoder_layer = nn.TransformerEncoderLayer(d_model = 768, #embedding_dimension\n",
    "                                                            nhead = 12,\n",
    "                                                            dim_feedforward = 3072, # mlp size\n",
    "                                                            dropout = 0.1,\n",
    "                                                            activation = 'gelu',\n",
    "                                                            batch_first = True,\n",
    "                                                            norm_first = True,\n",
    "                                                            )\n",
    "torch_transformer_encoder_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(torch_transformer_encoder_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(model = torch_transformer_encoder_layer,\n",
    "        input_size = [1, 197, 768],\n",
    "        col_names = ['input_size', 'output_size', 'num_params', 'trainable'],\n",
    "        col_width = 20,\n",
    "        row_settings = ['var_names'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a ViT class\n",
    "class ViT(nn.Module):\n",
    "    def __init__(self,\n",
    "                 img_size : int = 224,\n",
    "                 in_channels: int = 3,\n",
    "                 patch_size : int = 16,\n",
    "                 num_transformers_layers : int = 12,\n",
    "                 embedding_dim : int = 768,\n",
    "                 mlp_size : int = 3072,\n",
    "                 num_heads : int = 12,\n",
    "                 attn_dropout : int = 0,\n",
    "                 mlp_dropout : int = 0.1,\n",
    "                 embedding_dropout : int = 0.1,\n",
    "                 num_classes : int = 1000): # number of classes in our classification problem\n",
    "        super().__init__()\n",
    "\n",
    "        # make an assertion that the image size is compatible with the patch size\n",
    "        assert img_size % patch_size == 0 , f\"Image size must be divisible by patch size, image: {img_size}, patch_size: {patch_size}\"\n",
    "\n",
    "        # calculate the number of patches(height * weight / patch **2)\n",
    "        self.num_patches = (img_size * img_size) // patch_size **2\n",
    "\n",
    "        # create learnable class embedding\n",
    "        self.class_embedding = nn.Parameter(data = torch.randn(1, 1, embedding_dim),\n",
    "                                            requires_grad = True)\n",
    "\n",
    "        # create learnable position embedding\n",
    "        self.position_embedding = nn.Parameter(data = torch.randn(1, self.num_patches + 1, embedding_dim))\n",
    "\n",
    "        # create embedding dropout value\n",
    "        self.embedding_dropout = nn.Dropout(p = embedding_dropout)\n",
    "\n",
    "        # create patch embedding layer\n",
    "        self.patch_embedding = PatchEmbedding(in_channels= in_channels,\n",
    "                                              patch_size = patch_size,\n",
    "                                              embedding_dim = embedding_dim)\n",
    "        \n",
    "        # create the transformer encoder block\n",
    "                                                # here star(*) means all and then create list comprehension\n",
    "                                                # the below code means turn all the transformer encoder block into sequential layers \n",
    "        self.transformer_encoder = nn.Sequential(*[TransformerEncoderBlock(embedding_dim = embedding_dim,\n",
    "                                                                           num_heads= num_heads,\n",
    "                                                                           mlp_size= mlp_size,\n",
    "                                                                           mlp_dropout=mlp_dropout) for _ in range(num_transformers_layers)])\n",
    "        \n",
    "        # create classifier head(eqn 4)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.LayerNorm(normalized_shape = embedding_dim),\n",
    "            nn.Linear(in_features = embedding_dim,\n",
    "                      out_features = num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # get the batch size\n",
    "        batch_size = x.shape[0]\n",
    "\n",
    "        # create class token embedding and expand it to match the batch size\n",
    "        class_token = self.class_embedding.expand(batch_size, -1, -1) # -1 means to infer the dimensions\n",
    "\n",
    "        # create the patch embedding (equation 1)\n",
    "        x = self.patch_embedding(x)\n",
    "\n",
    "        # concat class token embedding and patch embedding(dquation 1)\n",
    "        x = torch.cat((class_token, x), dim = 1) #(batch_size, number_of_patches + 1, embedding_dim)\n",
    "\n",
    "        # add position embedding to class token and patch embedding\n",
    "\n",
    "        x = self.position_embedding + x\n",
    "\n",
    "        # apply dropout to patch embedding(\"directly after adding positional - to patch embeddings\")\n",
    "        x = self.embedding_dropout(x)\n",
    "\n",
    "        #pass position and patch embedding to transformer encoder\n",
    "        x = self.transformer_encoder(x)\n",
    "\n",
    "        # put 0th index logit through classifier (equation 4)\n",
    "        x = self.classifier(x[:, 0])\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "embedding_dim = 768\n",
    "class_embedding = nn.Parameter(data = torch.randn(1, 1, embedding_dim),\n",
    "                               requires_grad = True)\n",
    "class_embedding_expanded = class_embedding.expand(batch_size, -1, -1)\n",
    "print(class_embedding.shape)\n",
    "print(class_embedding_expanded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vit = ViT()\n",
    "vit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_image_tensor = torch.randn(1, 3, 224, 224)\n",
    "\n",
    "vit = ViT(num_classes=len(class_names))\n",
    "\n",
    "vit(rand_image_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create visual summary of our ViT model\n",
    "summary(model = ViT(num_classes = 1000),\n",
    "        input_size = (1, 3, 224, 224),\n",
    "        col_names = ['input_size', 'output_size', 'num_params', 'trainable'],\n",
    "        col_width = 20,\n",
    "        row_settings = ['var_names'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vit.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer\n",
    "optimizer = torch.optim.Adam(params = vit.parameters(),\n",
    "                             lr= 1e-3,\n",
    "                             betas = (0.9, 0.999),\n",
    "                             weight_decay = 0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss function/criterion\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training out model\n",
    "from going_modular import engine\n",
    "results = engine.train(model = vit,\n",
    "                       train_dataloader=train_dataloader,\n",
    "                       test_dataloader= test_dataloader,\n",
    "                       epochs = 10,\n",
    "                       optimizer = optimizer,\n",
    "                       loss_fn = loss_fn,\n",
    "                       device = device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
